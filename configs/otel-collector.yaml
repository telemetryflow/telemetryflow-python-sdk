# =============================================================================
# OpenTelemetry Collector Configuration (Community Default)
# =============================================================================
# TelemetryFlow Collector - Community Enterprise Observability Platform (CEOP)
# Copyright (c) 2024-2026 TelemetryFlow. All rights reserved.
#
# This is the DEFAULT community configuration using standard OTEL components.
# No TFO authentication required - compatible with any OTEL-compliant client.
#
# Usage: ./tfo-collector --config configs/otel-collector.yaml
#
# =============================================================================
# OTLP HTTP Endpoints (Standard OTEL - v1 only, NO AUTH)
# =============================================================================
#
#   HTTP Endpoints:
#     POST http://localhost:4318/v1/traces
#     POST http://localhost:4318/v1/metrics
#     POST http://localhost:4318/v1/logs
#
#   gRPC Endpoint:
#     localhost:4317
#
# =============================================================================
# Features Included:
# - Full OTLP receiver (gRPC + HTTP)
# - Traces, Metrics, Logs collection
# - Exemplars support via spanmetrics connector
# - Service graph generation for dependency mapping
# - Prometheus exporter with OpenMetrics support
# - Debug exporter for troubleshooting
# - Health check, zPages, pprof extensions
# =============================================================================

# =============================================================================
# EXTENSIONS - Additional collector capabilities
# =============================================================================
extensions:
  # Health check extension - provides /health endpoint
  health_check:
    endpoint: "0.0.0.0:13133"

  # zPages extension - debugging UI at /debug/tracez, /debug/pipelinez
  zpages:
    endpoint: "0.0.0.0:55679"

  # pprof extension - Go profiling at /debug/pprof
  pprof:
    endpoint: "0.0.0.0:1777"

# =============================================================================
# RECEIVERS - How telemetry data enters the collector
# =============================================================================
receivers:
  # OTLP receiver - standard OpenTelemetry Protocol
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
        max_recv_msg_size_mib: 4
        max_concurrent_streams: 100
        read_buffer_size: 524288
        write_buffer_size: 524288
        keepalive:
          server_parameters:
            max_connection_idle: 15s
            max_connection_age: 30s
            max_connection_age_grace: 5s
            time: 10s
            timeout: 5s
      http:
        endpoint: "0.0.0.0:4318"
        cors:
          allowed_origins:
            - "*"
          allowed_headers:
            - "*"
          max_age: 7200

  # Jaeger receiver for legacy Jaeger traces (uncomment to enable)
  # jaeger:
  #   protocols:
  #     grpc:
  #       endpoint: "0.0.0.0:14250"
  #     thrift_http:
  #       endpoint: "0.0.0.0:14268"
  #     thrift_compact:
  #       endpoint: "0.0.0.0:6831"
  #     thrift_binary:
  #       endpoint: "0.0.0.0:6832"

  # Zipkin receiver for legacy Zipkin traces (uncomment to enable)
  # zipkin:
  #   endpoint: "0.0.0.0:9411"

  # Prometheus scrape receiver (uncomment to enable)
  # prometheus:
  #   config:
  #     scrape_configs:
  #       - job_name: "node-exporter"
  #         scrape_interval: 15s
  #         static_configs:
  #           - targets: ["localhost:9100"]

  # Host metrics receiver (uncomment to enable)
  # hostmetrics:
  #   collection_interval: 30s
  #   scrapers:
  #     cpu:
  #     memory:
  #     disk:
  #     filesystem:
  #     load:
  #     network:

# =============================================================================
# PROCESSORS - How telemetry data is processed
# =============================================================================
processors:
  # Batch processor - groups telemetry for efficient export
  batch:
    timeout: 200ms
    send_batch_size: 8192
    send_batch_max_size: 0

  # Memory limiter - prevents OOM by limiting memory usage
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 25

  # Resource processor - adds/modifies resource attributes
  # To customize environment: export TELEMETRYFLOW_ENVIRONMENT=staging
  resource:
    attributes:
      - key: service.namespace
        value: telemetryflow
        action: upsert
      - key: deployment.environment
        value: production
        action: upsert

  # Resource detection (uncomment to auto-detect cloud/container resources)
  # resourcedetection:
  #   detectors: [env, system, docker, gcp, aws, azure]
  #   timeout: 5s
  #   override: false

  # Attributes processor (uncomment to modify span/metric attributes)
  # attributes:
  #   actions:
  #     - key: environment
  #       action: insert
  #       value: production

  # Tail sampling (uncomment for intelligent trace sampling)
  # tail_sampling:
  #   decision_wait: 10s
  #   num_traces: 100
  #   policies:
  #     - name: errors-policy
  #       type: status_code
  #       status_code:
  #         status_codes: [ERROR]
  #     - name: slow-traces-policy
  #       type: latency
  #       latency:
  #         threshold_ms: 1000

# =============================================================================
# CONNECTORS - Pipeline bridging for derived metrics
# =============================================================================
connectors:
  # Span metrics connector - derives metrics from traces with EXEMPLARS
  # Enables metrics-to-traces correlation in Grafana/Prometheus
  spanmetrics:
    histogram:
      explicit:
        buckets: [1ms, 5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 2.5s, 5s, 10s]
    dimensions:
      - name: http.method
        default: GET
      - name: http.status_code
      - name: http.route
      - name: rpc.method
      - name: rpc.service
    exemplars:
      enabled: true
    namespace: traces
    metrics_flush_interval: 15s

  # Service graph connector - builds service dependency graphs
  # Generates metrics for service-to-service relationships
  servicegraph:
    latency_histogram_buckets: [1ms, 5ms, 10ms, 25ms, 50ms, 100ms, 250ms, 500ms, 1s, 2.5s, 5s, 10s]
    dimensions:
      - http.method
      - http.status_code
    store:
      ttl: 2s
      max_items: 1000
    cache_loop: 1s
    store_expiration_loop: 2s
    virtual_node_peer_attributes:
      - db.system
      - messaging.system
      - rpc.service

# =============================================================================
# EXPORTERS - Where telemetry data is sent
# =============================================================================
exporters:
  # Debug exporter - logs telemetry for development/troubleshooting
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

  # Prometheus exporter - exposes metrics for Prometheus scraping
  # Supports exemplars for metrics-to-traces correlation
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: telemetryflow
    const_labels:
      collector: tfo-collector
    send_timestamps: true
    metric_expiration: 5m
    enable_open_metrics: true
    resource_to_telemetry_conversion:
      enabled: true

  # OTLP exporter (uncomment to forward to other backends)
  # otlp:
  #   endpoint: "your-backend:4317"
  #   tls:
  #     insecure: true
  #   headers:
  #     X-API-Key: "${env:API_KEY}"
  #   retry_on_failure:
  #     enabled: true
  #     initial_interval: 5s
  #     max_interval: 30s
  #   sending_queue:
  #     enabled: true
  #     num_consumers: 10
  #     queue_size: 1000

  # OTLP HTTP exporter (uncomment to forward via HTTP)
  # otlphttp:
  #   endpoint: "https://your-backend:4318"
  #   compression: gzip

  # Prometheus remote write (uncomment to push to Prometheus/Cortex/Mimir)
  # prometheusremotewrite:
  #   endpoint: "http://prometheus:9090/api/v1/write"
  #   tls:
  #     insecure: true
  #   resource_to_telemetry_conversion:
  #     enabled: true

  # File exporter (uncomment for local storage)
  # file:
  #   path: "/var/lib/tfo-collector/output.json"
  #   format: json
  #   rotation:
  #     max_megabytes: 100
  #     max_days: 7
  #     max_backups: 3

# =============================================================================
# SERVICE - Defines active components and pipelines
# =============================================================================
service:
  extensions: [health_check, zpages, pprof]

  pipelines:
    # -------------------------------------------------------------------------
    # Traces pipeline - receives traces, exports to debug and connectors
    # -------------------------------------------------------------------------
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [debug, spanmetrics, servicegraph]

    # -------------------------------------------------------------------------
    # Metrics pipeline - receives metrics from OTLP
    # -------------------------------------------------------------------------
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [debug, prometheus]

    # -------------------------------------------------------------------------
    # Metrics from traces - derived metrics from spanmetrics connector
    # These metrics include EXEMPLARS for correlation with traces
    # -------------------------------------------------------------------------
    metrics/spanmetrics:
      receivers: [spanmetrics]
      processors: [memory_limiter, batch]
      exporters: [prometheus]

    # -------------------------------------------------------------------------
    # Metrics from service graph - service dependency metrics
    # -------------------------------------------------------------------------
    metrics/servicegraph:
      receivers: [servicegraph]
      processors: [memory_limiter, batch]
      exporters: [prometheus]

    # -------------------------------------------------------------------------
    # Logs pipeline - receives logs from OTLP
    # -------------------------------------------------------------------------
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [debug]

  # Internal telemetry configuration
  telemetry:
    logs:
      level: info
      encoding: json

    metrics:
      level: detailed
      readers:
        - pull:
            exporter:
              prometheus:
                host: "0.0.0.0"
                port: 8888
